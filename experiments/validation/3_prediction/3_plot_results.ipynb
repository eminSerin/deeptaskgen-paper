{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import permutation_test, ttest_rel\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "sys.path.append(op.abspath(op.join(op.abspath(\"\"), \"..\")))\n",
    "from utils.utils import correlation_score\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"talk\", font_scale=1, rc={\"axes.labelpad\": 10})\n",
    "pd.set_option(\"display.float_format\", \"{:.3}\".format)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.float_format\", \"{:.3}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABS_PATH = sys.path[-1]\n",
    "RESULTS_PATH = op.join(\n",
    "    ABS_PATH, \"3_prediction/results\"\n",
    ")\n",
    "FIG_DIR = op.join(ABS_PATH, \"3_prediction/figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "PALETTE = {\n",
    "    \"Actual\": \"#283F94\",\n",
    "    \"Predicted\": \"#AE3033\",\n",
    "}\n",
    "\n",
    "CONTRASTS = (\n",
    "    \"REST\",\n",
    "    \"EMOTION FACES-SHAPES\",\n",
    "    \"GAMBLING REWARD\",\n",
    "    \"LANGUAGE MATH-STORY\",\n",
    "    \"RELATIONAL REL\",\n",
    "    \"SOCIAL TOM-RANDOM\",\n",
    "    \"WM 2BK-0BK\",\n",
    "    \"MOTOR AVG\",\n",
    ")\n",
    "\n",
    "CONTRASTS_MAP = {\n",
    "    \"ukb_actual\": (\n",
    "        \"rest\",\n",
    "        \"emotion_faces-shapes\",\n",
    "    ),\n",
    "    \"ukb_pred\": (\n",
    "        \"emotion_faces-shapes\",\n",
    "        \"gambling_reward\",\n",
    "        \"language_math-story\",\n",
    "        \"relational_rel\",\n",
    "        \"social_tom-random\",\n",
    "        \"wm_2bk-0bk\",\n",
    "        \"motor_avg\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "SCORE_FUNCS = {\n",
    "    \"age\": correlation_score,\n",
    "    \"fluid\": correlation_score,\n",
    "    \"sex\": balanced_accuracy_score,\n",
    "    \"strength\": correlation_score,\n",
    "    \"overall_health\": correlation_score,\n",
    "    \"alcohol_freq\": correlation_score,\n",
    "    \"depression\": balanced_accuracy_score,\n",
    "    \"neuroticism\": correlation_score,\n",
    "    \"GAD\": correlation_score,\n",
    "    \"PHQ\": correlation_score,\n",
    "    \"RDS\": correlation_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn of version warnings.\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Prepare dataframes for plotting.\n",
    "def results_files_dict():\n",
    "    pred_scores = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "    dsets = [\"ukb_actual\", \"ukb_pred\"]\n",
    "    for target in SCORE_FUNCS.keys():\n",
    "        for dset in CONTRASTS_MAP:\n",
    "            for cont in CONTRASTS_MAP[dset]:\n",
    "                cont = cont.replace(\" \", \"_\").lower()\n",
    "                try:\n",
    "                    fname = glob(\n",
    "                        op.join(\n",
    "                            RESULTS_PATH, dset,\n",
    "                            f\"{dset}_{target}_{cont}.pkl\",\n",
    "                        )\n",
    "                    )[0]\n",
    "                    pred_scores[target][dset][cont] = fname\n",
    "                except:\n",
    "                    print(f\"Could not find {dset}_{target}_{cont}.pkl\")\n",
    "                    pass\n",
    "    return pred_scores\n",
    "\n",
    "\n",
    "def compute_scores(pred_scores):\n",
    "    perm_scores = {}\n",
    "    cv_scores = {}\n",
    "    for target in pred_scores:\n",
    "        perm_scores[target] = {}\n",
    "        _scores = pd.DataFrame()\n",
    "        for dset in pred_scores[target]:\n",
    "            perm_scores[target][dset] = {}\n",
    "            for i, task in enumerate(pred_scores[target][dset]):\n",
    "                preds = pickle.load(open(pred_scores[target][dset][task], \"rb\"))\n",
    "                if i == 0:\n",
    "                    print(\n",
    "                        f\"Dataset: {dset}, Target: {target}, n = {np.hstack(preds[0]['y_true']).shape[0]}\"\n",
    "                    )\n",
    "                __sc = []\n",
    "                for pred in preds: \n",
    "                    __sc.append([SCORE_FUNCS[target](y_true, y_pred) for y_true, y_pred in zip(pred[\"y_true\"], pred[\"y_pred\"])])\n",
    "                perm_scores[target][dset][task] = __sc\n",
    "                tmp = pd.DataFrame(__sc[0], columns=[\"CV Score\"]).assign(\n",
    "                    Contrast=task.replace(\"_\", \"\\n\").upper(),\n",
    "                    Dataset=\"ukb\",\n",
    "                    Actual=\"Actual\" if \"pred\" not in dset else \"Predicted\",\n",
    "                )\n",
    "                _scores = pd.concat(\n",
    "                    [\n",
    "                        _scores,\n",
    "                        tmp,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "        cv_scores[target] = _scores\n",
    "    return cv_scores, perm_scores\n",
    "    \n",
    "cv_scores, perm_scores = compute_scores(results_files_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS\n",
    "def comp_perm_sig_cv(perms):\n",
    "    \"\"\"Compute permutation significance from given permutation scores.\"\"\"\n",
    "    cv = np.mean(perms[0])\n",
    "    perm_cv = [np.mean(p) for p in perms]\n",
    "    return cv, (perm_cv >= cv).mean()\n",
    "\n",
    "def comp_perm_sig_ttest(main, comp):\n",
    "    \"\"\"Compute permutation significance from given two sets of permutation scores.\"\"\"\n",
    "    t, _ = ttest_rel(main[0], comp[0])\n",
    "    perm_t = [ttest_rel(m, c)[0] for m, c in zip(main, comp)]\n",
    "    if t > 0:\n",
    "        return t, (perm_t >= t).mean()\n",
    "    else:\n",
    "        return t, (perm_t <= t).mean()\n",
    "\n",
    "\n",
    "def compare_tasks(perm_scores, target):\n",
    "    \"\"\"\n",
    "    Compare predicted and actual contrasts for a given target.\n",
    "\n",
    "    Args:\n",
    "        perm_scores (dict): Dictionary containing permutation scores for different contrasts.\n",
    "        target (str): Target contrast.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the comparison results.\n",
    "    \"\"\"\n",
    "    tasks_comp = []\n",
    "    for task in CONTRASTS_MAP[\"ukb_pred\"]:\n",
    "        main = perm_scores[target][\"ukb_pred\"][task]\n",
    "        for comp_task in CONTRASTS_MAP[\"ukb_actual\"]:\n",
    "            comp = perm_scores[target][\"ukb_actual\"][comp_task]\n",
    "            t, t_sig = comp_perm_sig_ttest(main, comp)\n",
    "            diff, diff_sig = comp_perm_sig_diff(main, comp)\n",
    "            tasks_comp.append(\n",
    "                {\n",
    "                    \"Predicted Contrast\": task,\n",
    "                    \"Actual Contrast\": comp_task,\n",
    "                    \"t\": t,\n",
    "                    \"p\": t_sig,\n",
    "                }\n",
    "            )\n",
    "    t, t_sig = comp_perm_sig_ttest(\n",
    "        perm_scores[target][\"ukb_actual\"][\"rest\"],\n",
    "        perm_scores[target][\"ukb_actual\"][\"emotion_faces-shapes\"],\n",
    "    )\n",
    "    tasks_comp.append(\n",
    "        {\n",
    "            \"Predicted Contrast\": \"rest\",\n",
    "            \"Actual Contrast\": \"emotion_faces-shapes\",\n",
    "            \"t\": t,\n",
    "            \"p\": t_sig,\n",
    "        }\n",
    "    )\n",
    "    return pd.DataFrame(tasks_comp)\n",
    "\n",
    "\n",
    "def print_permutation_significance(perm_scores, target):\n",
    "    \"\"\"\n",
    "    Prints the permutation significance for each task in each dataset.\n",
    "\n",
    "    Args:\n",
    "        perm_scores (dict): A dictionary containing permutation scores for each target, dataset, and task.\n",
    "        target (str): The target for which permutation significance is calculated.\n",
    "\n",
    "    Returns:\n",
    "        sigs (list): A list containing the permutation significance results.\n",
    "    \"\"\"\n",
    "    sigs = []\n",
    "    for dset in CONTRASTS_MAP.keys():\n",
    "        for task in CONTRASTS_MAP[dset]:\n",
    "            stat, sig = comp_perm_sig_cv(perm_scores[target][dset][task])\n",
    "            print(\n",
    "                f\"Permutation significance for {task} in {dset}: {stat:.3f}, p = {sig:.3f}\"\n",
    "            )\n",
    "            sigs.append({\n",
    "                \"Dataset\": dset.replace(\"_pred\", \"\"),\n",
    "                \"Contrast\": task.upper().replace(\"_\", \"\\n\"),\n",
    "                \"CV Score\": stat,\n",
    "                \"p\": sig,\n",
    "                \"Actual\": \"Actual\" if \"pred\" not in dset else \"Predicted\",\n",
    "            })\n",
    "    return pd.DataFrame(sigs)\n",
    "\n",
    "## Function for plotting CV scores\n",
    "def plot_cv_scores(target, ylim, sigs=None):\n",
    "    plt.figure(figsize=(11, 6), dpi=300)\n",
    "    plot_df = cv_scores[target].dropna()\n",
    "    if sigs is not None:\n",
    "        sigs[\"Dataset\"] = sigs[\"Dataset\"].replace({\"ukb_actual\": \"ukb\"})\n",
    "        plot_df = plot_df.merge(sigs.drop(\"CV Score\", axis=1), on=[\"Dataset\", \"Contrast\", \"Actual\"], how=\"left\")\n",
    "        plot_df['Alpha'] = plot_df['p'].apply(lambda x: 0.25 if x >= 0.05 else 0.75)\n",
    "    else:\n",
    "        plot_df['Alpha'] = 0.75\n",
    "    unique_contrasts = plot_df['Contrast'].unique()\n",
    "    unique_actual = plot_df['Actual'].unique()\n",
    "    for contrast in unique_contrasts:\n",
    "        for actual in unique_actual:\n",
    "            filter_df = plot_df[(plot_df['Contrast'] == contrast) & (plot_df['Actual'] == actual)]\n",
    "            if not filter_df.empty:\n",
    "                sns.pointplot(\n",
    "                    data=filter_df,\n",
    "                    x=\"Contrast\",\n",
    "                    y=\"CV Score\",\n",
    "                    hue=\"Actual\",\n",
    "                    errorbar=\"sd\",\n",
    "                    linestyles=\"\",\n",
    "                    palette=PALETTE,\n",
    "                    alpha=filter_df['Alpha'].iloc[0],  # Set alpha for each contrast\n",
    "                )\n",
    "    plt.ylim(ylim if ylim else target_ylim_map[target])  # Set y-axis limits\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.legend(loc=\"right\", bbox_to_anchor=(1.15, 0.5)).set_visible(False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(op.join(FIG_DIR, f\"{target}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "target = \"age\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (0.3, 0.7), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "target = \"sex\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (0.5, 1), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluid Intelligence\n",
    "target = \"fluid\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (-0.1, 0.301), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grip Strength\n",
    "target = \"strength\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (0, 0.601), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Health\n",
    "target = \"overall_health\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (-0.2, 0.2), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depression\n",
    "target = \"depression\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (0.4, 0.601), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alcohol Freq\n",
    "target = \"alcohol_freq\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, (-0.2, 0.2), sigs=sigs)\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuroticism\n",
    "target = \"neuroticism\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, sigs=sigs, ylim=(-0.2, 0.2))\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAD\n",
    "target = \"GAD\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, sigs=sigs, ylim=(-0.2, 0.2))\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ\n",
    "target = \"PHQ\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, sigs=sigs, ylim=(-0.2, 0.2))\n",
    "compare_tasks(perm_scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDS\n",
    "target = \"RDS\"\n",
    "sigs = print_permutation_significance(perm_scores, target)\n",
    "plot_cv_scores(target, sigs=sigs, ylim=(-0.2, 0.2))\n",
    "compare_tasks(perm_scores, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
